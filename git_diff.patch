diff --git a/.env b/.env
new file mode 100644
index 0000000..2f933cb
--- /dev/null
+++ b/.env
@@ -0,0 +1,3 @@
+PYTHONPATH=./src
+DEBUG=true
+ENV=dev
\ No newline at end of file
diff --git a/.gitignore b/.gitignore
index 4898e57..04c007d 100644
--- a/.gitignore
+++ b/.gitignore
@@ -7,3 +7,4 @@ artifacts/
 secrets.env
 .vscode/
 .pytest_cache/
+env/
\ No newline at end of file
diff --git a/Makefile b/Makefile
index d2a1975..c8571a9 100644
--- a/Makefile
+++ b/Makefile
@@ -1,44 +1,56 @@
-# Makefile
-.PHONY: db_up db_down db_logs db_cli test test-integration test-unit grafana_up env
-
-db_up:
-	docker compose up -d timescaledb
-	@echo "Waiting for TimescaleDB to start..."
-	@while ! docker exec -it tsdb pg_isready -U postgres; do \
-		echo "Waiting for TimescaleDB to be ready..."; \
+# Makefile for local dev: setup containers, run app, and run tests
+
+.PHONY: setup shutdown db_cli test test-integration test-unit run status
+
+# Ensure virtual environment is set up
+VENV_CHECK=test -f env/bin/activate || { echo \"❌ Virtualenv not found. Run 'make dev' first.\"; exit 1; }
+
+## Start all required containers (TimescaleDB, pgAdmin, Grafana, Loki)
+setup:
+	docker compose up -d timescaledb pgadmin grafana loki
+	@echo "⏳ Waiting for TimescaleDB to be ready..."
+	@while ! docker exec tsdb pg_isready -U postgres >/dev/null 2>&1; do \
+		echo "Waiting for TimescaleDB..."; \
 		sleep 2; \
 	done
-	@echo "TimescaleDB is ready!"
-	docker compose up -d pgadmin
-	@echo Use the following command to view the IP address of the TimescaleDB container:
-	@echo "docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' tsdb"
-	@echo "You can also connect using pgAdmin at http://localhost:8080"
-
-db_down:
-	docker compose stop timescaledb
-	docker compose stop pgadmin
-
-db_logs:
-	docker compose logs -f timescaledb
-
-grafana_up:
-	docker compose up -d loki grafana
-	@echo "Grafana is running at http://localhost:3000"
-	
-# quick psql shell (requires psql client installed inside WSL/Windows)
+	@echo "TimescaleDB is ready"
+	@echo "pgAdmin → http://localhost:8080"
+	@echo "Grafana → http://localhost:3000"
+
+## Stop all containers
+shutdown:
+	docker compose stop timescaledb pgadmin grafana loki
+	@echo "All containers stopped"
+
+## Open a psql shell to TimescaleDB
 db_cli:
 	psql "postgresql://postgres:postgres@localhost:5432/postgres"
-	@echo "Connected to TimescaleDB. Use \q to exit."
+	@echo "Use \\q to exit the shell"
+
+## Run the main program with virtual environment and PYTHONPATH=project root
+run:
+	@echo "Running application with PYTHONPATH=$(pwd)"
+	@bash -c "$(VENV_CHECK) && source env/bin/activate && export PYTHONPATH=$(pwd) && python3 src/main.py"
+
+## Run all tests with virtual environment
+test:
+	@bash -c "$(VENV_CHECK) && source env/bin/activate && pytest -v tests/"
+
+## Run only unit tests
+test-unit:
+	@bash -c "$(VENV_CHECK) && source env/bin/activate && pytest -v -m 'not integration' tests/"
+
+## Run only integration tests
+test-integration:
+	@bash -c "$(VENV_CHECK) && source env/bin/activate && pytest -v -m integration tests/"
 
-env:
-	@echo "To activate the virtual environment, run:"
-	@echo "source .venv/bin/activate"
 
-test:  ## Run all tests
-	pytest -v tests/
+## Show running container status
+status:
+	@docker compose ps --status=running
 
-test-unit:  ## Run only unit tests (if you tag with @pytest.mark.unit)
-	pytest -v -m "not integration" tests/
 
-test-integration:  ## Run only integration tests
-	pytest -v -m integration tests/
+## Run development startup script
+dev:
+	@echo "Running dev startup script..."
+	@./scripts/dev_startup.sh
\ No newline at end of file
diff --git a/README.md b/README.md
index 0eef08b..ca1c07f 100644
--- a/README.md
+++ b/README.md
@@ -1,79 +1,94 @@
+# Quant-Trad  
+*A modular, test-driven quantitative trading bot with strategy orchestration and live chart overlays*
 
+---
 
-# Quant-Trad 🚀  
-*A work-in-progress quantitative **trading bot** (autonomous execution coming soon)*
+## Vision
 
+Quant-Trad is being built to **trade autonomously**, combining clean data ingestion, flexible indicators, stateless signal rules, and configurable strategies.  
+Current focus is on structured feature extraction and signal generation, with backtesting and execution infrastructure in progress.
 
 ---
 
-## ✨ Vision
+## Core Architecture
 
-Quant-Trad is being built to **trade autonomously**.  
-Right now it focuses on clean data ingestion, robust indicator generation, and high-signal chart overlays.  
-Next milestones add strategy orchestration, parameter sweeps, and a live execution pipeline.
+| Layer           | Key Components | Notes |
+|----------------|----------------|-------|
+| **Data**        | `BaseProvider`, `AlpacaProvider`, `YahooProvider` | Unified OHLCV schema, optional TimescaleDB caching |
+| **Indicators**  | `PivotLevelIndicator`, `MarketProfileIndicator`, `TrendlineIndicator`, `VWAPIndicator` | Modular, composable, overlay-capable |
+| **Signals**     | Stateless signal rules (e.g. `breakout_rule`, `bounce_rule`) | Operate on indicator output + context |
+| **Strategies**  | `BaseStrategy`, `ReversalStrategy` | Orchestrates indicators and rules, produces structured signals |
+| **Visualization** | `ChartPlotter`, `OverlayRegistry`, `OverlayHandlers` | Candlesticks with high-signal overlays |
+| **Backtesting** | `Backtester`, `StrategyEngine` | Simulate strategy decisions over historical data |
+| **Monitoring**  | Loki (logs), Grafana (dashboards) | Docker services for system observability |
+| *(Planned)*     | Live Execution, Parameter Sweeps | Hooks for automated live trading and optimization |
 
 ---
 
-## 🏗️ Core Architecture (current)
+## Strategy Framework Overview
+
+| Component | Purpose |
+|----------|---------|
+| `Indicator` | Extracts features from OHLCV (levels, trendlines, VAH/VAL/POC) |
+| `SignalRule` | Stateless logic to evaluate market conditions |
+| `Strategy` | Registers indicators and rules, manages context, emits trade signals |
+| `DataContext` | Defines timeframe and range for each indicator instance |
+| `Signal` | Output object enriched with strategy and indicator metadata |
 
-| Layer | Key Components | Notes |
-|-------|----------------|-------|
-| **Data** | `BaseDataProvider`, `AlpacaProvider`, `YahooProvider` | Uniform OHLCV schema; optional TimescaleDB cache |
-| **Indicators** | `PivotLevelIndicator`, `MarketProfileIndicator` (TPO) | Implemented & tested |
-| *(Coming)* | `TrendlineIndicator`, `VWAPIndicator` | In development |
-| **Visualization** | `ChartPlotter` | Candles + volume + overlay lines |
-| *(Road-map)* | Strategy, Back-testing, Live Execution | Foundations laid, wiring next |
+Strategies can register the same indicator multiple times with different configurations and rules to support confluence across timeframes or techniques.
 
 ---
 
-## 📈 Indicators At-a-Glance
+## Indicators Summary
 
-| Indicator | Status | Purpose | Overlay Goodies |
-|-----------|--------|---------|-----------------|
-| **Pivot Level** | ✅ | Convert swing highs/lows into S/R levels | Role & timeframe colors, touch-points |
-| **Market Profile (TPO)** | ✅ | POC / VAH / VAL per session, VA merges | Dashed VA bands per session |
-| **Trendline** | 🔨 | Auto-detect dynamic trendlines | Continuous lines, breakout flags |
-| **VWAP** | 🔨 | 30-min volume profile & VA merges | Session bands, rolling POC anchor |
+| Indicator | Status | Purpose | Overlay Features |
+|-----------|--------|---------|------------------|
+| **Pivot Level** | Complete | Convert swing highs/lows into support/resistance zones | Timeframe coloring, touchpoint dots |
+| **Market Profile (TPO)** | Complete | Compute value areas and merged sessions | VA bands, dashed session overlays |
+| **Trendline** | Complete | Auto-detect dynamic trendlines from pivots | Line overlays, breakout regions |
+| **VWAP** | Complete | Compute value areas from intraday volume | Rolling session anchors, POC tracking |
 
 ---
 
-## ⚡ Makefile Commands
+## Makefile Commands
 
-| Target               | Description                                                             |
-|----------------------|-------------------------------------------------------------------------|
-| `make db_up`         | Spin up **TimescaleDB** and **pgAdmin** containers and wait until ready |
-| `make db_down`       | Stop the TimescaleDB / pgAdmin containers                               |
-| `make db_logs`       | Tail TimescaleDB logs (`Ctrl-C` to quit)                                |
-| `make db_cli`        | Open a `psql` shell at `postgres://postgres:postgres@localhost:5432/postgres` |
-| `make test`          | Run the full pytest suite                                               |
-| `make test-unit`     | Run only unit tests (`-m "not integration"`)                           |
-| `make test-integration` | Run only tests tagged `@pytest.mark.integration`                     |
+| Target            | Description |
+|-------------------|-------------|
+| `make setup`      | Start TimescaleDB, pgAdmin, Grafana, and Loki containers |
+| `make shutdown`   | Stop all containers |
+| `make db_cli`     | Open a `psql` shell to TimescaleDB |
+| `make run`        | Run the app with `PYTHONPATH` set to the project root |
+| `make test`       | Run all tests |
+| `make test-unit`  | Run only unit tests |
+| `make test-integration` | Run only integration tests |
+| `make status`     | Show status of running containers |
+| `make dev`        | Run the local dev startup script (`scripts/dev_startup.sh`) |
 
+---
 
-## ⚡ Quick Start-up
+## Quick Start
 
-> **Prerequisites**  
-> • Python 3.10+ with `venv`  
-> • [Docker Desktop](https://www.docker.com/products/docker-desktop/) running (needed for the TimescaleDB + pgAdmin containers)  
-> • GNU Make (pre-installed on macOS/Linux; Windows users can use the Git-Bash version or **WSL**)
+**Prerequisites**
+- Python 3.10+
+- Docker Desktop
+- GNU Make (comes with macOS/Linux; Windows users can use Git Bash or WSL)
 
 ```bash
-# 0) clone the repo
+# Clone the repo
 git clone --branch develop https://github.com/elijahbrookss/quant-trad.git
 cd quant-trad
 
-# 1) spin-up TimescaleDB (+ pgAdmin) in Docker
-make db_up 
+# Create dev setup
+make dev
 
-# 2) create / activate virtual-env and install deps
-python -m venv .venv && source .venv/bin/activate
-pip install -r requirements.txt
+# Start core services (TimescaleDB, pgAdmin, Grafana, Loki)
+make setup 
 
-# 3) run the full test-suite
+# Run tests
 make test            # or: make test-unit / make test-integration
 
-# 4) (optional) open a psql shell
-make db_cli          # \q to exit
+# Launch TimescaleDB CLI (optional)
+make db_cli
 
-# 5) shut containers down when finished
-make db_down
\ No newline at end of file
+# Shut down services when done
+make shutdown
\ No newline at end of file
diff --git a/classes/ChartPlotter.py b/classes/ChartPlotter.py
deleted file mode 100644
index c48eb09..0000000
--- a/classes/ChartPlotter.py
+++ /dev/null
@@ -1,148 +0,0 @@
-import pandas as pd
-import mplfinance as mpf
-from typing import Optional
-from classes.Logger import logger
-import os
-from typing import List, Any
-import matplotlib.pyplot as plt
-from typing import Set, Tuple
-from matplotlib import patches
-from classes.indicators.config import DataContext
-from itertools import groupby
-from classes.OverlayRegistry import get_overlay_handler
-import classes.OverlayHandlers
-
-
-class ChartPlotter:
-
-    @staticmethod
-    def plot_ohlc(
-        df: pd.DataFrame,
-        title: str,
-        ctx: DataContext,
-        datasource: str,
-        show_volume: bool = True,
-        chart_type: str = "candle",
-        output_base: str = "output",
-        output_subdir: str = "misc",
-        legend_entries: Set[Tuple[str, str]] = None,
-        overlays: Optional[List[Any]] = None,
-        file_name: Optional[str] = None
-    ):
-        """
-        Plots OHLC data using mplfinance, scoped to a given DataContext.
-        """
-        try:
-            ctx.validate()
-            start = pd.to_datetime(ctx.start).tz_localize("UTC")
-            end = pd.to_datetime(ctx.end).tz_localize("UTC")
-
-            logger.debug("Index sample: %s → %s", df.index.min(), df.index.max())
-
-            if df is None or df.empty:
-                logger.warning("No data to plot for given symbol and date range.")
-                raise ValueError("Cannot plot: DataFrame is empty or None.")
-
-            required_columns = {'timestamp', 'open', 'high', 'low', 'close'}
-            if not required_columns.issubset(df.columns):
-                raise ValueError(f"DataFrame must contain columns: {required_columns}, but found: {df.columns}")
-
-            df = df.copy()
-            df['timestamp'] = pd.to_datetime(df['timestamp'])
-            df.set_index('timestamp', inplace=True)
-            df.index = pd.to_datetime(df.index)
-
-            logger.debug("Filtering for %s → %s", start, end)
-            df = df[(df.index >= start) & (df.index <= end)]
-
-            if df.empty:
-                raise ValueError(f"No data to plot after filtering from {start} to {end}.")
-
-            output_dir = os.path.join(output_base, output_subdir)
-            os.makedirs(output_dir, exist_ok=True)
-
-            start_end_time= f"{start.strftime('%Y-%m-%d')}_{end.strftime('%Y-%m-%d')}"
-            file_name = f"{file_name}_{start_end_time}.png" if file_name else f"{ctx.symbol}_{ctx.interval}_{start_end_time}.png"
-            file_path = os.path.join(output_dir, file_name)
-
-            fig_width = min(10 + len(df.index) * 0.03, 30)
-            figsize = (fig_width, 6)
-
-            addplot_specs, other_specs = ChartPlotter._split_overlays(overlays)
-
-            fig, axes = mpf.plot(
-                df,
-                type=chart_type,
-                volume=show_volume and "volume" in df.columns,
-                title=title,
-                style="yahoo",
-                addplot=addplot_specs,
-                returnfig=True,
-                figsize=figsize
-            )
-
-            price_ax = axes[0]
-            ChartPlotter._dispatch_overlays(df, price_ax, other_specs)
-
-            if legend_entries:
-                handles = [
-                    patches.Patch(color=color, label=label)
-                    for label, color in sorted(legend_entries)
-                ]
-                axes[0].legend(handles=handles, loc="upper left", fontsize=8)
-
-            fig.savefig(file_path, dpi=300, bbox_inches="tight")
-            logger.info("Chart saved to %s", file_path)
-
-        except IndexError:
-            logger.warning("IndexError: No data points in filtered date range (%s to %s)", ctx.start, ctx.end)
-            raise ValueError(f"No data available in the date range {ctx.start} to {ctx.end}.")
-
-        except Exception as e:
-            logger.exception("Charting failed: %s", str(e))
-
-    @staticmethod
-    def _split_overlays(overlays):
-        """
-        Splits the overlays into:
-          - addplot_specs: list of mplfinance addplot objects
-          - other_specs:   list of (kind, specs_list) for the rest
-        """
-        addplot_specs = []
-        other_by_kind = {}
-
-        for item in overlays or []:
-            # Determine its kind
-            if isinstance(item, dict) and "kind" in item:
-                kind = item["kind"]
-            else:
-                kind = "addplot"
-
-            if kind == "addplot":
-                # Unwrap the real addplot object
-                if isinstance(item, dict) and "plot" in item:
-                    addplot_specs.append(item["plot"])
-                else:
-                    # raw mplfinance addplot passed straight through
-                    addplot_specs.append(item)
-            else:
-                # Group the rest by kind
-                other_by_kind.setdefault(kind, []).append(item)
-
-        # Turn the dict into a list of tuples
-        other_specs = list(other_by_kind.items())
-        print("rect specs:", [group for group in other_specs if group[0]=="rect"])
-
-        return addplot_specs, other_specs
-
-    @staticmethod
-    def _dispatch_overlays(df, price_ax, other_specs):
-        """
-        Dispatches overlays to the appropriate handler based on kind.
-        """
-        for kind, spec_list in other_specs:
-            handler = get_overlay_handler(kind)
-            if handler:
-                handler(df, price_ax, spec_list)
-            else:
-                logger.warning("No handler found for overlay kind: %s", kind)
\ No newline at end of file
diff --git a/classes/Logger.py b/classes/Logger.py
deleted file mode 100644
index 4727938..0000000
--- a/classes/Logger.py
+++ /dev/null
@@ -1,25 +0,0 @@
-import logging
-from .logging_utils import LokiHandler, ExcludeLoggerFilter
-
-LOG_FMT = "%(asctime)s %(levelname)-5s %(filename)s:%(lineno)d | %(message)s"
-logging.basicConfig(level=logging.DEBUG, format=LOG_FMT)
-
-LOKI_URL = "http://localhost:3100"
-LOKI_LABELS = {"app": "quant_trad", "env": "dev"}
-
-loki_handler = LokiHandler(url=LOKI_URL, labels=LOKI_LABELS, timeout=1.0)
-loki_handler.setLevel(logging.DEBUG)
-loki_handler.setFormatter(logging.Formatter(LOG_FMT))
-loki_handler.addFilter(ExcludeLoggerFilter(["urllib3", "requests", "loki.internal", "font_manager"]))
-
-root_logger = logging.getLogger()
-root_logger.addHandler(loki_handler)
-
-logger = logging.getLogger(__name__)
-
-# logging.basicConfig(
-#     level=logging.DEBUG,
-#     format="%(asctime)s - %(levelname)s - %(filename)s:%(funcName)s:%(lineno)d - %(message)s"
-# )
-
-# logger = logging.getLogger(__name__)
\ No newline at end of file
diff --git a/classes/OverlayHandlers.py b/classes/OverlayHandlers.py
deleted file mode 100644
index d773ae5..0000000
--- a/classes/OverlayHandlers.py
+++ /dev/null
@@ -1,23 +0,0 @@
-import matplotlib.dates as mdates
-from matplotlib.patches import Rectangle
-from .OverlayRegistry import register_overlay_handler
-
-@register_overlay_handler("addplot")
-def handle_addplot(df, price_ax, specs):
-    return specs
-
-@register_overlay_handler("rect")
-def handle_rectangle(df, price_ax, specs):
-    last_num = mdates.date2num(df.index[-1])
-    for r in specs:
-        start_num = mdates.date2num(r["start"])
-        rect = Rectangle(
-            (start_num,  r["val"]),
-            width      = last_num - start_num,
-            height     = r["vah"] - r["val"],
-            facecolor  = r.get("color", "gray"),
-            alpha      = r.get("alpha", 0.2),
-            edgecolor  = None,
-            zorder = 1
-        )
-        price_ax.add_patch(rect)
\ No newline at end of file
diff --git a/classes/engines/__pycache__/Backtester.cpython-310.pyc b/classes/engines/__pycache__/Backtester.cpython-310.pyc
deleted file mode 100644
index 8032194..0000000
Binary files a/classes/engines/__pycache__/Backtester.cpython-310.pyc and /dev/null differ
diff --git a/classes/engines/__pycache__/Backtester.cpython-312.pyc b/classes/engines/__pycache__/Backtester.cpython-312.pyc
deleted file mode 100644
index 2066616..0000000
Binary files a/classes/engines/__pycache__/Backtester.cpython-312.pyc and /dev/null differ
diff --git a/classes/engines/__pycache__/StrategyEngine.cpython-310.pyc b/classes/engines/__pycache__/StrategyEngine.cpython-310.pyc
deleted file mode 100644
index c2b5566..0000000
Binary files a/classes/engines/__pycache__/StrategyEngine.cpython-310.pyc and /dev/null differ
diff --git a/classes/engines/__pycache__/StrategyEngine.cpython-312.pyc b/classes/engines/__pycache__/StrategyEngine.cpython-312.pyc
deleted file mode 100644
index 48fae2f..0000000
Binary files a/classes/engines/__pycache__/StrategyEngine.cpython-312.pyc and /dev/null differ
diff --git a/classes/indicators/MarketProfileIndicator.py b/classes/indicators/MarketProfileIndicator.py
deleted file mode 100644
index adeb0ea..0000000
--- a/classes/indicators/MarketProfileIndicator.py
+++ /dev/null
@@ -1,274 +0,0 @@
-import numpy as np
-import pandas as pd
-from typing import Dict, List, Tuple, Set
-import matplotlib.dates as mdates
-from matplotlib.patches import Rectangle
-
-from mplfinance.plotting import make_addplot
-
-from classes.Logger import logger
-from classes.indicators.BaseIndicator import BaseIndicator
-from classes.indicators.config import DataContext
-
-
-class MarketProfileIndicator(BaseIndicator):
-    """
-    Computes daily market profile (TPO) to identify Point of Control (POC),
-    Value Area High (VAH), and Value Area Low (VAL), and provides plotting overlays.
-    """
-    NAME = "market_profile"
-
-    def __init__(
-        self,
-        df: pd.DataFrame,
-        bin_size: float = 0.1,
-        mode: str = "tpo"
-    ):
-        """
-        :param df: OHLCV DataFrame indexed by timestamp.
-        :param bin_size: price bucket size for TPO histogram.
-        :param mode: profile mode (only 'tpo' supported today).
-        """
-        super().__init__(df)
-        self.bin_size = bin_size
-        self.mode = mode
-        # Compute raw daily profiles on initialization
-        self.daily_profiles: List[Dict[str, float]] = self._compute_daily_profiles()
-        self.merged_profiles: List[Dict[str, float]] = []
-
-    @classmethod
-    def from_context(
-        cls,
-        provider,
-        ctx: DataContext,
-        bin_size: float = 0.1,
-        mode: str = "tpo",
-        interval: str = "30m"
-    ) -> "MarketProfileIndicator":
-        """
-        Fetches OHLCV from provider and constructs the indicator.
-        Raises ValueError if no data is available.
-        """
-        ctx = DataContext(
-            symbol=ctx.symbol,
-            start=ctx.start,
-            end=ctx.end,
-            interval=interval
-        )
-        ctx.validate()
-
-        df = provider.get_ohlcv(ctx)
-        if df is None or df.empty:
-            raise ValueError(
-                f"MarketProfileIndicator: No data available for {ctx.symbol} [{ctx.interval}] after ingest"
-            )
-
-        return cls(df=df, bin_size=bin_size, mode=mode)
-
-    def _compute_daily_profiles(self) -> List[Dict[str, float]]:
-        """
-        Build daily TPO profiles: POC, VAH, VAL, plus session timestamps.
-        """
-        df = self.df.copy()
-        df.index = pd.to_datetime(df.index, utc=True)
-        profiles: List[Dict[str, float]] = []
-
-        # Group rows by calendar date
-        grouped = df.groupby(df.index.date)
-        for session_date, group in grouped:
-            tpo_hist = self._build_tpo_histogram(group)
-            value_area = self._extract_value_area(tpo_hist)
-
-            first_ts = group.index.min()
-            last_ts = group.index.max()
-            value_area.update({
-                "date": pd.to_datetime(session_date),
-                "start_date": first_ts,
-                "end_date": last_ts
-            })
-
-            profiles.append(value_area)
-            logger.debug(
-                "Profile for %s: POC=%.2f, VAH=%.2f, VAL=%.2f",
-                session_date,
-                value_area["POC"],
-                value_area["VAH"],
-                value_area["VAL"]
-            )
-
-        return profiles
-
-    def _build_tpo_histogram(self, data: pd.DataFrame) -> Dict[float, int]:
-        """
-        Count how many bars visit each price bucket defined by bin_size.
-        :param data: intraday DataFrame for one session.
-        :return: mapping of price bucket -> count of TPO occurrences.
-        """
-        tpo_counts: Dict[float, int] = {}
-        for _, row in data.iterrows():
-            low, high = row["low"], row["high"]
-            prices = np.arange(low, high + self.bin_size, self.bin_size)
-            for price in prices:
-                bucket = round(price / self.bin_size) * self.bin_size
-                tpo_counts[bucket] = tpo_counts.get(bucket, 0) + 1
-        return tpo_counts
-
-    def _extract_value_area(self, tpo_hist: Dict[float, int]) -> Dict[str, float]:
-        """
-        From the TPO histogram, compute:
-          - POC: price with highest count
-          - VAH: upper bound of 70% cumulative TPO
-          - VAL: lower bound of 70% cumulative TPO
-        """
-        total = sum(tpo_hist.values())
-        if total == 0:
-            return {"POC": None, "VAH": None, "VAL": None}
-
-        # sort buckets by descending count
-        sorted_buckets = sorted(tpo_hist.items(), key=lambda item: item[1], reverse=True)
-        poc_price = sorted_buckets[0][0]
-
-        cumulative = 0
-        va_prices: List[float] = []
-        threshold = 0.7 * total
-        for price, count in sorted_buckets:
-            cumulative += count
-            va_prices.append(price)
-            if cumulative >= threshold:
-                break
-
-        return {
-            "POC": poc_price,
-            "VAH": max(va_prices),
-            "VAL": min(va_prices)
-        }
-
-    @staticmethod
-    def _calculate_overlap(
-        val1: float,
-        vah1: float,
-        val2: float,
-        vah2: float
-    ) -> float:
-        """
-        Compute the overlap ratio between two value areas,
-        normalized by the range of the second area.
-        """
-        low = max(val1, val2)
-        high = min(vah1, vah2)
-        overlap = max(0.0, high - low)
-        range2 = vah2 - val2
-        return overlap / range2 if range2 > 0 else 0.0
-
-    def merge_value_areas(
-        self,
-        threshold: float = 0.6,
-        min_merge: int = 2
-    ) -> List[Dict[str, float]]:
-        """
-        Combine consecutive daily profiles whose value areas overlap
-        at least `threshold` fraction, requiring at least `min_merge` days.
-        """
-        merged: List[Dict[str, float]] = []
-        profiles = self.daily_profiles
-        i, n = 0, len(profiles)
-
-        while i < n:
-            base = profiles[i]
-            merged_val = base["VAL"]
-            merged_vah = base["VAH"]
-            start_ts = base["start_date"]
-            end_ts = base["end_date"]
-            poc_list = [base["POC"]] if base.get("POC") is not None else []
-            count = 1
-            j = i + 1
-
-            while j < n:
-                next_prof = profiles[j]
-                overlap = self._calculate_overlap(
-                    merged_val,
-                    merged_vah,
-                    next_prof["VAL"],
-                    next_prof["VAH"]
-                )
-                if overlap < threshold:
-                    break
-
-                merged_val = min(merged_val, next_prof["VAL"])
-                merged_vah = max(merged_vah, next_prof["VAH"])
-                end_ts = next_prof["end_date"]
-                if next_prof.get("POC") is not None:
-                    poc_list.append(next_prof["POC"])
-                count += 1
-                j += 1
-
-            if count >= min_merge:
-                avg_poc = sum(poc_list) / len(poc_list) if poc_list else None
-                merged.append({
-                    "start_date": start_ts,
-                    "end_date": end_ts,
-                    "VAL": merged_val,
-                    "VAH": merged_vah,
-                    "POC": avg_poc
-                })
-
-            i = j
-
-        self.merged_profiles = merged
-        return merged
-
-    def to_overlays(
-        self,
-        plot_df: pd.DataFrame,
-        use_merged: bool = True
-    ) -> Tuple[List, Set[Tuple[str, str]]]:
-        """
-        Emit two kinds of overlay specs:
-        • kind="rect" → persistent VAH/VAL zones  
-        • kind="addplot" → POC horizontal line
-        """
-        profiles = self.merged_profiles if use_merged else self.daily_profiles
-        if not profiles:
-            logger.warning("No profiles to generate overlays.")
-            return [], set()
-
-        overlays: List[dict] = []
-        legend_entries: Set[Tuple[str, str]] = set()
-        full_idx = plot_df.index
-
-        for prof in profiles:
-            start_ts = prof["start_date"]
-            val      = prof["VAL"]
-            vah      = prof["VAH"]
-            poc      = prof.get("POC")
-
-            # 1) persistent rectangle for VA zone
-            overlays.append({
-                "kind":  "rect",
-                "start": start_ts,
-                "val":   val,
-                "vah":   vah,
-                "color": "gray",
-                "alpha": 0.2
-            })
-            legend_entries.add(("Value Area", "gray"))
-
-            # 2) optional POC line as an addplot
-            if poc is not None:
-                # mask out everything before start_ts
-                poc_series = pd.Series(index=full_idx, dtype=float)
-                poc_series.loc[full_idx >= start_ts] = poc
-
-                ap = make_addplot(
-                    poc_series,
-                    color="orange",
-                    width=1.0,
-                    linestyle="--"
-                )
-                overlays.append({
-                    "kind": "addplot",
-                    "plot": ap
-                })
-                legend_entries.add(("POC", "orange"))
-
-        return overlays, legend_entries
diff --git a/classes/indicators/__pycache__/BaseIndicator.cpython-310.pyc b/classes/indicators/__pycache__/BaseIndicator.cpython-310.pyc
deleted file mode 100644
index 5f0b603..0000000
Binary files a/classes/indicators/__pycache__/BaseIndicator.cpython-310.pyc and /dev/null differ
diff --git a/classes/indicators/__pycache__/BaseIndicator.cpython-312.pyc b/classes/indicators/__pycache__/BaseIndicator.cpython-312.pyc
deleted file mode 100644
index daca707..0000000
Binary files a/classes/indicators/__pycache__/BaseIndicator.cpython-312.pyc and /dev/null differ
diff --git a/classes/indicators/__pycache__/LevelsIndicator.cpython-310.pyc b/classes/indicators/__pycache__/LevelsIndicator.cpython-310.pyc
deleted file mode 100644
index db89dc0..0000000
Binary files a/classes/indicators/__pycache__/LevelsIndicator.cpython-310.pyc and /dev/null differ
diff --git a/classes/indicators/__pycache__/LevelsIndicator.cpython-312.pyc b/classes/indicators/__pycache__/LevelsIndicator.cpython-312.pyc
deleted file mode 100644
index a180961..0000000
Binary files a/classes/indicators/__pycache__/LevelsIndicator.cpython-312.pyc and /dev/null differ
diff --git a/classes/indicators/__pycache__/MarketProfileIndicator.cpython-310.pyc b/classes/indicators/__pycache__/MarketProfileIndicator.cpython-310.pyc
deleted file mode 100644
index 61f9a3b..0000000
Binary files a/classes/indicators/__pycache__/MarketProfileIndicator.cpython-310.pyc and /dev/null differ
diff --git a/classes/indicators/__pycache__/MarketProfileIndicator.cpython-312.pyc b/classes/indicators/__pycache__/MarketProfileIndicator.cpython-312.pyc
deleted file mode 100644
index 1e6c248..0000000
Binary files a/classes/indicators/__pycache__/MarketProfileIndicator.cpython-312.pyc and /dev/null differ
diff --git a/classes/indicators/__pycache__/PivotLevelIndicator.cpython-312.pyc b/classes/indicators/__pycache__/PivotLevelIndicator.cpython-312.pyc
deleted file mode 100644
index 3c5bda3..0000000
Binary files a/classes/indicators/__pycache__/PivotLevelIndicator.cpython-312.pyc and /dev/null differ
diff --git a/classes/indicators/__pycache__/PivotLevelOverlay.cpython-312.pyc b/classes/indicators/__pycache__/PivotLevelOverlay.cpython-312.pyc
deleted file mode 100644
index 853d139..0000000
Binary files a/classes/indicators/__pycache__/PivotLevelOverlay.cpython-312.pyc and /dev/null differ
diff --git a/classes/indicators/__pycache__/TrendlineIndicator.cpython-312.pyc b/classes/indicators/__pycache__/TrendlineIndicator.cpython-312.pyc
deleted file mode 100644
index 5639dda..0000000
Binary files a/classes/indicators/__pycache__/TrendlineIndicator.cpython-312.pyc and /dev/null differ
diff --git a/classes/indicators/__pycache__/VWAPIndicator.cpython-310.pyc b/classes/indicators/__pycache__/VWAPIndicator.cpython-310.pyc
deleted file mode 100644
index 03a1128..0000000
Binary files a/classes/indicators/__pycache__/VWAPIndicator.cpython-310.pyc and /dev/null differ
diff --git a/classes/indicators/__pycache__/VWAPIndicator.cpython-312.pyc b/classes/indicators/__pycache__/VWAPIndicator.cpython-312.pyc
deleted file mode 100644
index 025936b..0000000
Binary files a/classes/indicators/__pycache__/VWAPIndicator.cpython-312.pyc and /dev/null differ
diff --git a/data_providers/__pycache__/alpaca.cpython-310.pyc b/data_providers/__pycache__/alpaca.cpython-310.pyc
deleted file mode 100644
index 3134375..0000000
Binary files a/data_providers/__pycache__/alpaca.cpython-310.pyc and /dev/null differ
diff --git a/data_providers/__pycache__/alpaca.cpython-312.pyc b/data_providers/__pycache__/alpaca.cpython-312.pyc
deleted file mode 100644
index 39dbb20..0000000
Binary files a/data_providers/__pycache__/alpaca.cpython-312.pyc and /dev/null differ
diff --git a/data_providers/__pycache__/base.cpython-310.pyc b/data_providers/__pycache__/base.cpython-310.pyc
deleted file mode 100644
index 54a8c1a..0000000
Binary files a/data_providers/__pycache__/base.cpython-310.pyc and /dev/null differ
diff --git a/data_providers/__pycache__/base.cpython-312.pyc b/data_providers/__pycache__/base.cpython-312.pyc
deleted file mode 100644
index a55b8af..0000000
Binary files a/data_providers/__pycache__/base.cpython-312.pyc and /dev/null differ
diff --git a/data_providers/__pycache__/yahoo.cpython-310.pyc b/data_providers/__pycache__/yahoo.cpython-310.pyc
deleted file mode 100644
index 562aa3b..0000000
Binary files a/data_providers/__pycache__/yahoo.cpython-310.pyc and /dev/null differ
diff --git a/data_providers/__pycache__/yahoo.cpython-312.pyc b/data_providers/__pycache__/yahoo.cpython-312.pyc
deleted file mode 100644
index 943b0d9..0000000
Binary files a/data_providers/__pycache__/yahoo.cpython-312.pyc and /dev/null differ
diff --git a/main.py b/main.py
deleted file mode 100644
index 8e6dd46..0000000
--- a/main.py
+++ /dev/null
@@ -1,52 +0,0 @@
-import pandas as pd
-from pathlib import Path
-from classes.Logger import logger
-
-from classes.ChartPlotter import ChartPlotter
-from classes.indicators.MarketProfileIndicator import MarketProfileIndicator
-from classes.indicators.PivotLevelIndicator import PivotLevelIndicator
-from classes.indicators.VWAPIndicator import VWAPIndicator
-from classes.indicators.TrendlineIndicator import TrendlineIndicator
-
-from data_providers.alpaca_provider import AlpacaProvider
-from classes.indicators.config import DataContext
-
-
-symbol = "CL"
-provider = AlpacaProvider()
-
-ctx = DataContext(
-    symbol="CL",
-    start="2025-06-01",
-    end="2025-06-30",
-    interval="15m"
-)
-
-def get_overlays(plot_df: pd.DataFrame):
-    mpi = MarketProfileIndicator.from_context(
-        provider=provider,
-        ctx=ctx,
-        bin_size=0.5,      # can adjust bin size as desired
-        mode="tpo",
-        interval="30m"
-    )
-    merged = mpi.merge_value_areas()
-
-    return mpi.to_overlays(plot_df, use_merged=True)
-
-
-def show():
-    plot_df = provider.get_ohlcv(ctx)
-    overlays, legend_keys = get_overlays(plot_df)
-
-    logger.debug([overlay["kind"] for overlay in overlays])
-
-    provider.plot_ohlcv(
-        plot_ctx=ctx,
-        title="Integration Test – Market Profile (CL 30m)",
-        overlays=overlays,
-        legend_entries=legend_keys,
-        show_volume=True
-    )
-
-show()
\ No newline at end of file
diff --git a/output/integration_tests/market_profile/CL_30m_2025-06-01_2025-06-30.png b/output/integration_tests/market_profile/CL_30m_2025-06-01_2025-06-30.png
index 54dc29d..2f31f82 100644
Binary files a/output/integration_tests/market_profile/CL_30m_2025-06-01_2025-06-30.png and b/output/integration_tests/market_profile/CL_30m_2025-06-01_2025-06-30.png differ
diff --git a/output/integration_tests/pivot_levels/CL_15m_2025-05-15_2025-06-13.png b/output/integration_tests/pivot_levels/CL_15m_2025-05-15_2025-06-13.png
index 17e64d2..f5a35dd 100644
Binary files a/output/integration_tests/pivot_levels/CL_15m_2025-05-15_2025-06-13.png and b/output/integration_tests/pivot_levels/CL_15m_2025-05-15_2025-06-13.png differ
diff --git a/output/integration_tests/trendlines/CL_15m_2025-05-15_2025-06-13.png b/output/integration_tests/trendlines/CL_15m_2025-05-15_2025-06-13.png
index 62c43fe..c884eda 100644
Binary files a/output/integration_tests/trendlines/CL_15m_2025-05-15_2025-06-13.png and b/output/integration_tests/trendlines/CL_15m_2025-05-15_2025-06-13.png differ
diff --git a/output/integration_tests/vwap_bands/CL_15m_2025-05-15_2025-05-30.png b/output/integration_tests/vwap_bands/CL_15m_2025-05-15_2025-05-30.png
index 9d42645..aabb2ff 100644
Binary files a/output/integration_tests/vwap_bands/CL_15m_2025-05-15_2025-05-30.png and b/output/integration_tests/vwap_bands/CL_15m_2025-05-15_2025-05-30.png differ
diff --git a/output/misc/CL_15m_2025-06-01_2025-06-30.png b/output/misc/CL_15m_2025-06-01_2025-06-30.png
index b40fa94..2715bfd 100644
Binary files a/output/misc/CL_15m_2025-06-01_2025-06-30.png and b/output/misc/CL_15m_2025-06-01_2025-06-30.png differ
diff --git a/output/misc/CL_15m_2025-06-01_2025-07-13.png b/output/misc/CL_15m_2025-06-01_2025-07-13.png
new file mode 100644
index 0000000..7b83c4e
Binary files /dev/null and b/output/misc/CL_15m_2025-06-01_2025-07-13.png differ
diff --git a/output/misc/CL_15m_2025-06-01_2025-07-15.png b/output/misc/CL_15m_2025-06-01_2025-07-15.png
new file mode 100644
index 0000000..6172226
Binary files /dev/null and b/output/misc/CL_15m_2025-06-01_2025-07-15.png differ
diff --git a/output/misc/CL_15m_2025-07-01_2025-07-15.png b/output/misc/CL_15m_2025-07-01_2025-07-15.png
new file mode 100644
index 0000000..1f34585
Binary files /dev/null and b/output/misc/CL_15m_2025-07-01_2025-07-15.png differ
diff --git a/output/misc/ES_15m_2025-06-01_2025-07-13.png b/output/misc/ES_15m_2025-06-01_2025-07-13.png
new file mode 100644
index 0000000..e5a69eb
Binary files /dev/null and b/output/misc/ES_15m_2025-06-01_2025-07-13.png differ
diff --git a/output/misc/GS_15m_2025-06-01_2025-07-13.png b/output/misc/GS_15m_2025-06-01_2025-07-13.png
new file mode 100644
index 0000000..5b4e406
Binary files /dev/null and b/output/misc/GS_15m_2025-06-01_2025-07-13.png differ
diff --git a/pytest.ini b/pytest.ini
index cc97290..cc304d2 100644
--- a/pytest.ini
+++ b/pytest.ini
@@ -1,6 +1,9 @@
 [pytest]
-pythonpath = .
+pythonpath = ./src
 addopts = --strict-markers
 markers =
     integration: mark test as an integration test deselect with -m "not integration"
-    unit: mark test as a unit test deselect with -m "not unit"
\ No newline at end of file
+    unit: mark test as a unit test deselect with -m "not unit"
+
+log_cli = true
+log_cli_level = DEBUG
\ No newline at end of file
diff --git a/scripts/dev_startup.sh b/scripts/dev_startup.sh
new file mode 100755
index 0000000..2c289b6
--- /dev/null
+++ b/scripts/dev_startup.sh
@@ -0,0 +1,62 @@
+#!/bin/bash
+
+set -e
+
+# -------------------------------
+# 🔧 Dev Setup Script
+# -------------------------------
+echo "Initializing development environment..."
+
+# 2. Create virtual environment if missing
+if [ ! -d "env" ]; then
+  echo "Creating virtual environment..."
+  python3 -m venv env || { echo "Failed to create virtual environment. This must be resolved manually."; exit 1; }
+else
+  echo "Virtual environment already exists."
+fi
+
+# 3. Activate virtual environment
+source env/bin/activate || { echo "Failed to activate virtual environment. This must be resolved manually."; exit 1; }
+
+# Ensure we're in the virtual environment
+if [[ "$VIRTUAL_ENV" == "" ]]; then
+  echo "Virtual environment not active. Aborting dependency install."
+  exit 1
+fi
+
+
+# 4. Install dependencies
+if [ ! -f "requirements.txt" ]; then
+  echo "Missing requirements.txt. Please ensure it exists at the project root."
+  exit 1
+fi
+
+echo "Installing dependencies..."
+pip install -r requirements.txt || { echo "pip install failed. Please check the packages in requirements.txt."; exit 1; }
+
+# 5. Load .env (optional)
+if [ -f ".env" ]; then
+  echo "Loading environment variables from .env"
+  export $(grep -v '^#' .env | xargs)
+else
+  echo ".env file not found. Proceeding without it. You may create one to set flags like DEBUG or ENV."
+fi
+
+# 6. Load secrets.env (optional)
+if [ -f "secrets.env" ]; then
+  echo "🔐 Loading secrets from secrets.env"
+  export $(grep -v '^#' secrets.env | xargs)
+else
+  echo "secrets.env not found. You can create one to define private keys or API credentials."
+fi
+
+# 7. Validate PYTHONPATH
+if [[ ":$PYTHONPATH:" != *":$(pwd)/src:"* ]]; then
+  export PYTHONPATH=$(pwd)
+  echo "PYTHONPATH set to: $PYTHONPATH"
+fi
+
+# 8. Finish
+echo "Dev environment ready. Suggested next step:"
+echo "source env/bin/activate"
+echo "python src/main.py"
\ No newline at end of file
diff --git a/src/core/chart_plotter.py b/src/core/chart_plotter.py
new file mode 100644
index 0000000..a3fb9ee
--- /dev/null
+++ b/src/core/chart_plotter.py
@@ -0,0 +1,223 @@
+import pandas as pd
+import mplfinance as mpf
+from typing import Optional, List, Any, Set, Tuple
+from core.logger import logger
+import os
+import matplotlib.pyplot as plt
+from matplotlib import patches
+from indicators.config import DataContext
+from core.overlay_registry import get_overlay_handler
+import core.overlay_handlers
+import numpy as np
+
+
+class ChartPlotter:
+
+    @staticmethod
+    def plot_ohlc(
+        df: pd.DataFrame,
+        title: str,
+        ctx: DataContext,
+        datasource: str,
+        show_volume: bool = True,
+        chart_type: str = "candle",
+        output_base: str = "output",
+        output_subdir: str = "misc",
+        legend_entries: Set[Tuple[str, str]] = None,
+        overlays: Optional[List[Any]] = None,
+        file_name: Optional[str] = None
+    ):
+        try:
+            logger.info(
+                "Starting plot_ohlc for symbol=%s, interval=%s, chart_type=%s",
+                ctx.symbol, ctx.interval, chart_type
+            )
+            logger.debug("Overlays provided: %s", overlays)
+
+            ctx.validate()
+            start, end = ChartPlotter._get_plot_range(ctx)
+            df = ChartPlotter._prepare_dataframe(df)
+            ChartPlotter._log_index_info(df)
+
+            df = ChartPlotter._filter_dataframe(df, start, end)
+            ChartPlotter._ensure_not_empty(df, start, end)
+
+            file_path = ChartPlotter._get_output_path(
+                output_base, output_subdir, ctx, file_name, start, end
+            )
+
+            figsize = ChartPlotter._get_figsize(df)
+            logger.debug("Figure size set to: %s", figsize)
+
+            addplot_specs, other_specs = ChartPlotter._split_overlays(overlays)
+            addplot_specs = ChartPlotter._clean_addplots(addplot_specs)
+
+            ChartPlotter._log_addplot_details(addplot_specs)
+
+            fig, axes = mpf.plot(
+                df,
+                type=chart_type,
+                volume=show_volume and "volume" in df.columns,
+                title=title,
+                style="charles",
+                addplot=addplot_specs,
+                returnfig=True,
+                figsize=figsize
+            )
+            logger.info("mplfinance plot created.")
+
+            price_ax = axes[0]
+            ChartPlotter._dispatch_overlays(df, price_ax, other_specs)
+
+            logger.debug("Axis limits after overlays: xlim=%s, ylim=%s", price_ax.get_xlim(), price_ax.get_ylim())
+
+            if legend_entries:
+                ChartPlotter._add_legend(axes[0], legend_entries)
+
+            # autoscale 
+            # price_ax.autoscale_view()
+
+            fig.savefig(file_path, dpi=300, bbox_inches="tight")
+            logger.info("Chart saved to %s", file_path)
+
+        except IndexError:
+            logger.warning("IndexError: No data points in filtered date range (%s to %s)", ctx.start, ctx.end)
+            raise ValueError(f"No data available in the date range {ctx.start} to {ctx.end}.")
+
+        except Exception as e:
+            logger.exception("Charting failed: %s", str(e))
+
+    @staticmethod
+    def _get_plot_range(ctx):
+        start = pd.to_datetime(ctx.start).tz_localize("UTC")
+        end = pd.to_datetime(ctx.end).tz_localize("UTC")
+        return start, end
+
+    @staticmethod
+    def _prepare_dataframe(df):
+        if df is None or df.empty:
+            logger.warning("No data to plot for given symbol and date range.")
+            raise ValueError("Cannot plot: DataFrame is empty or None.")
+
+        required_columns = {'timestamp', 'open', 'high', 'low', 'close'}
+        if not required_columns.issubset(df.columns):
+            logger.error("Missing required columns. Required: %s, Found: %s", required_columns, df.columns)
+            raise ValueError(f"DataFrame must contain columns: {required_columns}, but found: {df.columns}")
+
+        df = df.copy()
+        df['timestamp'] = pd.to_datetime(df['timestamp'])
+        df.set_index('timestamp', inplace=True)
+        df.index = pd.to_datetime(df.index)
+        return df
+
+    @staticmethod
+    def _log_index_info(df):
+        logger.debug("Plotting DataFrame index type: %s", type(df.index))
+        logger.debug("Plotting DataFrame index sample: %s", df.index[:5])
+
+    @staticmethod
+    def _filter_dataframe(df, start, end):
+        logger.debug("Filtering DataFrame for range: %s → %s", start, end)
+        return df[(df.index >= start) & (df.index <= end)]
+
+    @staticmethod
+    def _ensure_not_empty(df, start, end):
+        if df.empty:
+            logger.warning("No data to plot after filtering from %s to %s.", start, end)
+            raise ValueError(f"No data to plot after filtering from {start} to {end}.")
+
+    @staticmethod
+    def _get_output_path(output_base, output_subdir, ctx, file_name, start, end):
+        output_dir = os.path.join(output_base, output_subdir)
+        os.makedirs(output_dir, exist_ok=True)
+        logger.debug("Output directory ensured: %s", output_dir)
+        start_end_time = f"{start.strftime('%Y-%m-%d')}_{end.strftime('%Y-%m-%d')}"
+        file_name = f"{file_name}_{start_end_time}.png" if file_name else f"{ctx.symbol}_{ctx.interval}_{start_end_time}.png"
+        file_path = os.path.join(output_dir, file_name)
+        logger.info("Output file path: %s", file_path)
+        return file_path
+
+    @staticmethod
+    def _get_figsize(df):
+        fig_width = min(10 + len(df.index) * 0.03, 30)
+        return (fig_width, 6)
+
+    @staticmethod
+    def _clean_addplots(addplot_specs):
+        cleaned_addplots = []
+        for i, spec in enumerate(addplot_specs):
+            label = spec.get("label", f"Overlay #{i}")
+            if spec.get("scatter", False):
+                x, y = spec.get("x"), spec.get("y")
+                if not x or not y:
+                    logger.warning("Skipping scatter (%s) with missing or empty x/y.", label)
+                    continue
+            else:
+                data = spec.get("data")
+                if data is None or (isinstance(data, pd.Series) and (data.empty or data.isna().all())):
+                    logger.warning("Skipping line (%s) with missing or empty data.", label)
+                    continue
+                if isinstance(data, (list, np.ndarray)) and (len(data) == 0 or all(pd.isna(v) for v in data)):
+                    logger.warning("Skipping line (%s): empty or all-NaN array.", label)
+                    continue
+            cleaned_addplots.append(spec)
+        logger.info("Final cleaned addplot overlays: %d", len(cleaned_addplots))
+        return cleaned_addplots
+
+    @staticmethod
+    def _log_addplot_details(addplot_specs):
+        for i, spec in enumerate(addplot_specs):
+            label = spec.get("label", f"Overlay #{i}")
+            if not spec.get("scatter", False):
+                data = spec.get("data")
+                if isinstance(data, pd.Series):
+                    logger.debug("Line overlay %s head:\n%s", label, data.head(3))
+                    logger.debug(
+                        "Line overlay %s stats: len=%d, nulls=%d, isna.all=%s",
+                        label, len(data), data.isna().sum(), data.isna().all()
+                    )
+
+    @staticmethod
+    def _split_overlays(overlays):
+        addplot_specs = []
+        other_by_kind = {}
+        logger.debug("Splitting overlays: total=%d", len(overlays) if overlays else 0)
+        for item in overlays or []:
+            kind = item.get("kind") if isinstance(item, dict) and "kind" in item else "addplot"
+            if kind == "addplot":
+                if isinstance(item, dict) and "plot" in item:
+                    addplot_specs.append(item["plot"])
+                else:
+                    addplot_specs.append(item)
+            else:
+                other_by_kind.setdefault(kind, []).append(item)
+
+        other_specs = list(other_by_kind.items())
+        rect_count = len(other_by_kind.get("rect", []))
+        logger.debug(
+            "Overlay split: addplot=%d, rects=%d, other_kinds=%d",
+            len(addplot_specs),
+            rect_count,
+            len(other_by_kind) - (1 if "rect" in other_by_kind else 0)
+        )
+        return addplot_specs, other_specs
+
+    @staticmethod
+    def _dispatch_overlays(df, price_ax, other_specs):
+        logger.debug("Dispatching overlays: %d kinds", len(other_specs))
+        for kind, spec_list in other_specs:
+            handler = get_overlay_handler(kind)
+            if handler:
+                logger.debug("Dispatching %d overlays of kind '%s'", len(spec_list), kind)
+                handler(df, price_ax, spec_list)
+            else:
+                logger.warning("No handler found for overlay kind: %s", kind)
+
+    @staticmethod
+    def _add_legend(ax, legend_entries):
+        handles = [
+            patches.Patch(color=color, label=label)
+            for label, color in sorted(legend_entries)
+        ]
+        ax.legend(handles=handles, loc="upper left", fontsize=8)
+        logger.debug("Legend added with %d entries.", len(handles))
\ No newline at end of file
diff --git a/src/core/logger.py b/src/core/logger.py
new file mode 100644
index 0000000..fe2990a
--- /dev/null
+++ b/src/core/logger.py
@@ -0,0 +1,28 @@
+import logging
+from utils.logging_utils import LokiHandler, ExcludeLoggerFilter
+from dotenv import load_dotenv
+import os
+
+load_dotenv("secrets.env")
+load_dotenv(".env")
+
+debug_mode = True if os.getenv("DEBUG", "false").lower() == "true" else False
+
+LOG_FMT = "%(asctime)s %(levelname)-5s %(filename)s:%(lineno)d | %(message)s"
+logging.basicConfig(level=logging.DEBUG, format=LOG_FMT)
+
+LOKI_URL = "http://localhost:3100"
+LOKI_LABELS = {"app": "quant_trad", "env": os.getenv("ENV", "dev") }
+
+loki_handler = LokiHandler(url=LOKI_URL, labels=LOKI_LABELS, timeout=1.0)
+loki_handler.setLevel(logging.DEBUG if debug_mode else logging.INFO)
+loki_handler.setFormatter(logging.Formatter(LOG_FMT))
+loki_handler.addFilter(ExcludeLoggerFilter(["urllib3", "requests", "loki.internal"]))
+
+root_logger = logging.getLogger()
+root_logger.addHandler(loki_handler)
+
+logger = logging.getLogger(__name__)
+
+logging.getLogger("matplotlib.font_manager").setLevel(logging.WARNING)
+logging.getLogger("urllib3.connectionpool").setLevel(logging.WARNING)
\ No newline at end of file
diff --git a/src/core/overlay_handlers.py b/src/core/overlay_handlers.py
new file mode 100644
index 0000000..92b0224
--- /dev/null
+++ b/src/core/overlay_handlers.py
@@ -0,0 +1,67 @@
+import matplotlib.dates as mdates
+from matplotlib.patches import Rectangle
+import matplotlib.transforms as mtransforms
+from .overlay_registry import register_overlay_handler
+from core.logger import logger
+import pandas as pd
+import mplfinance as mpf
+import numpy as np
+
+@register_overlay_handler("addplot")
+def handle_addplot(df, price_ax, specs):
+    logger.debug("Handling addplot overlays: count=%d", len(specs))
+    return specs
+
+@register_overlay_handler("line")
+def handle_line(df, price_ax, specs):
+    logger.info("Handling line overlays: count=%d", len(specs))
+    logger.debug("Axis limits before overlays: xlim=%s, ylim=%s", price_ax.get_xlim(), price_ax.get_ylim())
+    
+    return specs
+
+@register_overlay_handler("scatter")
+def handle_scatter(df, price_ax, specs):
+    logger.info("Handling scatter overlays: count=%d", len(specs))
+    logger.debug("Axis limits before overlays: xlim=%s, ylim=%s", price_ax.get_xlim(), price_ax.get_ylim())
+    
+    return specs
+
+@register_overlay_handler("rect")
+def handle_rectangle(df, price_ax, specs):
+    logger.info("Handling rectangle overlays: count=%d", len(specs))
+    logger.debug("Axis limits before overlays: xlim=%s, ylim=%s", price_ax.get_xlim(), price_ax.get_ylim())
+    
+    x_vals = np.arange(len(df.index))  # Positional X values (0, 1, 2, ..., N)
+    
+    for idx, r in enumerate(specs):
+        start = r.get("start", df.index[0])
+        end = r.get("end", df.index[-1])
+        val = r["val"]
+        vah = r["vah"]
+        color = r.get("color", "gray")
+        alpha = r.get("alpha", 0.2)
+        
+        mask = (df.index >= start) & (df.index <= end)
+        x_range = x_vals[mask]
+        val_vals = np.full_like(x_range, val, dtype=float)
+        vah_vals = np.full_like(x_range, vah, dtype=float)
+
+        price_ax.fill_between(
+            x=x_range,
+            y1=val_vals,
+            y2=vah_vals,
+            color="black",
+            alpha=alpha,
+            linewidth=2,
+            edgecolor=color,
+            zorder=2
+        )
+
+        logger.debug(
+            "Rectangle %d: fill_between points=%d, VAL=%.2f, VAH=%.2f, color=%s, alpha=%.2f",
+            idx, len(x_range), val, vah, color, alpha
+        )
+
+    logger.debug("Axis limits after overlays: xlim=%s, ylim=%s", price_ax.get_xlim(), price_ax.get_ylim())
+
+
diff --git a/classes/OverlayRegistry.py b/src/core/overlay_registry.py
similarity index 100%
rename from classes/OverlayRegistry.py
rename to src/core/overlay_registry.py
diff --git a/data_providers/alpaca_provider.py b/src/data_providers/alpaca_provider.py
similarity index 98%
rename from data_providers/alpaca_provider.py
rename to src/data_providers/alpaca_provider.py
index 23c6401..7cc7d3d 100644
--- a/data_providers/alpaca_provider.py
+++ b/src/data_providers/alpaca_provider.py
@@ -6,7 +6,7 @@ from alpaca.data.historical import StockHistoricalDataClient
 from alpaca.data.requests import StockBarsRequest
 from alpaca.data.timeframe import TimeFrame, TimeFrameUnit
 from alpaca.data.enums import DataFeed
-from classes.Logger import logger
+from core.logger import logger
 from .base_provider import DataSource
 from .base_provider import BaseDataProvider
 
diff --git a/data_providers/base_provider.py b/src/data_providers/base_provider.py
similarity index 96%
rename from data_providers/base_provider.py
rename to src/data_providers/base_provider.py
index 75fdd0f..dc1694c 100644
--- a/data_providers/base_provider.py
+++ b/src/data_providers/base_provider.py
@@ -6,9 +6,9 @@ import os
 from dotenv import load_dotenv
 from sqlalchemy import create_engine, text
 from sqlalchemy.exc import SQLAlchemyError
-from classes.Logger import logger
-from classes.indicators.config import DataContext
-from classes.ChartPlotter import ChartPlotter
+from core.logger import logger
+from indicators.config import DataContext
+from core.chart_plotter import ChartPlotter
 
 load_dotenv("secrets.env")
 
diff --git a/data_providers/yahoo_provider.py b/src/data_providers/yahoo_provider.py
similarity index 92%
rename from data_providers/yahoo_provider.py
rename to src/data_providers/yahoo_provider.py
index 85e9080..d98a0a9 100644
--- a/data_providers/yahoo_provider.py
+++ b/src/data_providers/yahoo_provider.py
@@ -1,9 +1,9 @@
 import datetime as dt
 import pandas as pd
 import yfinance as yf
-from classes.Logger import logger
-from .base import DataSource
-from .base import BaseDataProvider
+from core.logger import logger
+from .base_provider import DataSource
+from .base_provider import BaseDataProvider
 
 class YahooFinanceProvider(BaseDataProvider):
     def fetch_from_api(
diff --git a/classes/engines/Backtester.py b/src/engines/backtester.py
similarity index 97%
rename from classes/engines/Backtester.py
rename to src/engines/backtester.py
index cb0fbc0..904603d 100644
--- a/classes/engines/Backtester.py
+++ b/src/engines/backtester.py
@@ -1,8 +1,8 @@
 import pandas as pd
 from typing import List, Dict, Optional
 
-from classes.engines.StrategyEngine import StrategyEngine
-from classes.indicators.BaseIndicator import BaseIndicator
+from engines.strategy_engine import StrategyEngine
+from indicators.base import BaseIndicator
 from dataclasses import dataclass
 
 class Trade:
diff --git a/classes/engines/StrategyEngine.py b/src/engines/strategy_engine.py
similarity index 98%
rename from classes/engines/StrategyEngine.py
rename to src/engines/strategy_engine.py
index c6f1f9e..882de66 100644
--- a/classes/engines/StrategyEngine.py
+++ b/src/engines/strategy_engine.py
@@ -1,7 +1,7 @@
 from typing import List, Tuple, Optional
 import pandas as pd
 
-from classes.indicators.BaseIndicator import BaseIndicator
+from indicators.base import BaseIndicator
 
 
 class StrategyEngine:
diff --git a/classes/indicators/BaseIndicator.py b/src/indicators/base.py
similarity index 80%
rename from classes/indicators/BaseIndicator.py
rename to src/indicators/base.py
index b7cf668..fe3d41a 100644
--- a/classes/indicators/BaseIndicator.py
+++ b/src/indicators/base.py
@@ -3,7 +3,7 @@ import pandas as pd
 
 from pathlib import Path
 from typing import Tuple
-from classes.Logger import logger  # noqa – existing helper
+from core.logger import logger
 
 ARTIFACT_ROOT = Path("artifacts")
 
@@ -13,6 +13,7 @@ class BaseIndicator:
     NAME: str = "base"
 
     def __init__(self, df: pd.DataFrame):
+        logger.debug("Initializing %s with DataFrame of shape %s", self.NAME, df.shape)
         self.df = df.copy()
         self.result = None  # indicator‑specific output artefact
         self.score: float | None = None  # placeholder for future use
@@ -20,11 +21,13 @@ class BaseIndicator:
     # ------------------------------------------------------------------
     def compute(self):  # noqa: D401 – imperative style
         """Populate *self.result* with the indicator calculation."""
+        logger.info("compute() called on %s", self.NAME)
         raise NotImplementedError
 
     # ------------------------------------------------------------------
     def plot(self) -> Path:
         """Render PNG artifact and return its filesystem path."""
+        logger.info("plot() called on %s", self.NAME)
         raise NotImplementedError
 
     # ------------------------------------------------------------------
@@ -32,6 +35,7 @@ class BaseIndicator:
     # the same visual baseline without duplicating styling rules.
     @staticmethod
     def _init_price_ax(df: pd.DataFrame, title: str) -> Tuple[plt.Figure, plt.Axes]:
+        logger.debug("Initializing price axis for plot: %s (df shape: %s)", title, df.shape)
         plt.style.use("dark_background")
         fig, ax = plt.subplots(figsize=(12, 6))
         ax.plot(df.index, df["Close"], label="Close", color="cyan", alpha=0.6)
@@ -39,6 +43,7 @@ class BaseIndicator:
         ax.set_xlabel("Date", color="white")
         ax.set_ylabel("Price", color="white")
         ax.grid(alpha=0.2, color="gray")
+        logger.debug("Price axis initialized for: %s", title)
         return fig, ax
 
     # ------------------------------------------------------------------
@@ -46,6 +51,7 @@ class BaseIndicator:
         folder = ARTIFACT_ROOT / self.NAME
         folder.mkdir(parents=True, exist_ok=True)
         path = folder / filename
+        logger.debug("Saving figure for %s to %s", self.NAME, path)
         fig.savefig(path, dpi=300, bbox_inches="tight", facecolor="black")
         plt.close(fig)
         logger.info("%s plot saved → %s", self.NAME, path)
diff --git a/classes/indicators/config.py b/src/indicators/config.py
similarity index 100%
rename from classes/indicators/config.py
rename to src/indicators/config.py
diff --git a/src/indicators/market_profile.py b/src/indicators/market_profile.py
new file mode 100644
index 0000000..5cd42fd
--- /dev/null
+++ b/src/indicators/market_profile.py
@@ -0,0 +1,352 @@
+import numpy as np
+import pandas as pd
+from typing import Dict, List, Tuple, Set
+import matplotlib.dates as mdates
+from matplotlib.patches import Rectangle
+from mplfinance.plotting import make_addplot
+import logging
+
+from core.logger import logger
+from .base import BaseIndicator
+from .config import DataContext
+
+
+class MarketProfileIndicator(BaseIndicator):
+    """
+    Computes daily market profile (TPO) to identify Point of Control (POC),
+    Value Area High (VAH), and Value Area Low (VAL), and provides plotting overlays.
+    """
+    NAME = "market_profile"
+
+    def __init__(self, df: pd.DataFrame, bin_size: float = 0.1, mode: str = "tpo"):
+        super().__init__(df)
+        self.bin_size = bin_size
+        self.mode = mode
+        # Compute raw daily profiles on initialization
+        self.daily_profiles = self._compute_daily_profiles()
+        self.merged_profiles = []
+
+    @classmethod
+    def from_context(cls, provider, ctx: DataContext, bin_size: float = 0.1, mode: str = "tpo", interval: str = "30m"):
+        """
+        Fetches OHLCV from provider and constructs the indicator.
+        Raises ValueError if no data is available.
+        """
+        ctx = DataContext(symbol=ctx.symbol, start=ctx.start, end=ctx.end, interval=interval)
+        ctx.validate()
+
+        df = provider.get_ohlcv(ctx)
+        if df is None or df.empty:
+            raise ValueError(f"MarketProfileIndicator: No data available for {ctx.symbol} [{ctx.interval}] after ingest")
+
+        return cls(df=df, bin_size=bin_size, mode=mode)
+
+    def _compute_daily_profiles(self) -> List[Dict[str, float]]:
+        """
+        Build daily TPO profiles: POC, VAH, VAL, plus session timestamps.
+        """
+        df = self.df.copy()
+        df.index = pd.to_datetime(df.index, utc=True)
+        profiles = []
+
+        logger.info("Starting daily profile computation for %d sessions", len(np.unique(df.index.date)))
+        # Group rows by calendar date
+        grouped = df.groupby(df.index.date)
+        for session_date, group in grouped:
+            logger.debug("Processing session: %s, bars: %d", session_date, len(group))
+            tpo_hist = self._build_tpo_histogram(group)
+            value_area = self._extract_value_area(tpo_hist)
+
+            value_area.update({
+                "date": pd.to_datetime(session_date),
+                "start_date": group.index.min(),
+                "end_date": group.index.max()
+            })
+
+            profiles.append(value_area)
+            logger.info("Profile for %s: POC=%.2f, VAH=%.2f, VAL=%.2f", session_date, value_area["POC"], value_area["VAH"], value_area["VAL"])
+
+        logger.info("Completed daily profile computation. Total profiles: %d", len(profiles))
+        return profiles
+
+    def _build_tpo_histogram(self, data: pd.DataFrame) -> Dict[float, int]:
+        """
+        Count how many bars visit each price bucket defined by bin_size.
+        :param data: intraday DataFrame for one session.
+        :return: mapping of price bucket -> count of TPO occurrences.
+        """
+        tpo_counts = {}
+        logger.debug("Building TPO histogram for session with %d bars", len(data))
+        for _, row in data.iterrows():
+            low, high = row["low"], row["high"]
+            prices = np.arange(low, high + self.bin_size, self.bin_size)
+            for price in prices:
+                bucket = round(price / self.bin_size) * self.bin_size
+                tpo_counts[bucket] = tpo_counts.get(bucket, 0) + 1
+        logger.debug("Built TPO histogram with %d buckets", len(tpo_counts))
+        return tpo_counts
+
+    def _extract_value_area(self, tpo_hist: Dict[float, int]) -> Dict[str, float]:
+        """
+        From the TPO histogram, compute:
+          - POC: price with highest count
+          - VAH: upper bound of 70% cumulative TPO
+          - VAL: lower bound of 70% cumulative TPO
+        """
+        total = sum(tpo_hist.values())
+        if total == 0:
+            logger.warning("TPO histogram is empty, cannot extract value area.")
+            return {"POC": None, "VAH": None, "VAL": None}
+
+        # sort buckets by descending count
+        sorted_buckets = sorted(tpo_hist.items(), key=lambda item: item[1], reverse=True)
+        poc_price = sorted_buckets[0][0]
+
+        cumulative = 0
+        va_prices = []
+        threshold = 0.7 * total
+        for price, count in sorted_buckets:
+            cumulative += count
+            va_prices.append(price)
+            if cumulative >= threshold:
+                break
+
+        logger.debug("Extracted value area: POC=%.2f, VAH=%.2f, VAL=%.2f, total TPO=%d", poc_price, max(va_prices), min(va_prices), total)
+        return {"POC": poc_price, "VAH": max(va_prices), "VAL": min(va_prices)}
+
+    def merge_value_areas(self, threshold: float = 0.6, min_merge: int = 2) -> List[Dict[str, float]]:
+        """
+        Combine consecutive daily profiles whose value areas overlap
+        at least `threshold` fraction, requiring at least `min_merge` days.
+        """
+        merged = []
+        profiles = self.daily_profiles
+        i, n = 0, len(profiles)
+
+        logger.info("Starting merge of value areas: threshold=%.2f, min_merge=%d", threshold, min_merge)
+
+        while i < n:
+            base = profiles[i]
+            merged_val, merged_vah = base["VAL"], base["VAH"]
+            start_ts, end_ts = base["start_date"], base["end_date"]
+            poc_list = [base["POC"]] if base.get("POC") is not None else []
+            count = 1
+            j = i + 1
+
+            logger.debug("Merging from profile %d (start: %s)", i, start_ts)
+            while j < n:
+                next_prof = profiles[j]
+                overlap = self._calculate_overlap(merged_val, merged_vah, next_prof["VAL"], next_prof["VAH"])
+                logger.debug("Checking overlap with profile %d: overlap=%.2f (threshold=%.2f)", j, overlap, threshold)
+                if overlap < threshold:
+                    logger.debug("Overlap below threshold, stopping merge at profile %d", j)
+                    break
+                merged_val = min(merged_val, next_prof["VAL"])
+                merged_vah = max(merged_vah, next_prof["VAH"])
+                end_ts = next_prof["end_date"]
+                if next_prof.get("POC") is not None:
+                    poc_list.append(next_prof["POC"])
+                count += 1
+                j += 1
+
+            if count >= min_merge:
+                avg_poc = sum(poc_list) / len(poc_list) if poc_list else None
+                merged.append({
+                    "start": start_ts,
+                    "end": end_ts,
+                    "VAL": merged_val,
+                    "VAH": merged_vah,
+                    "POC": avg_poc
+                })
+                logger.info("Merged %d profiles: [%s → %s], VAL=%.2f, VAH=%.2f, avg POC=%.2f", count, start_ts, end_ts, merged_val, merged_vah, avg_poc if avg_poc else float('nan'))
+            else:
+                logger.debug("Merge group too small (%d < %d), skipping", count, min_merge)
+            i = j
+
+        self.merged_profiles = merged
+        logger.info("Completed merging. Total merged profiles: %d", len(merged))
+        return merged
+
+    def to_overlays(self, plot_df: pd.DataFrame, use_merged: bool = True) -> Tuple[List, Set[Tuple[str, str]]]:
+        """
+        Emit two kinds of overlay specs:
+        • kind="rect" → persistent VAH/VAL zones  
+        • kind="addplot" → POC horizontal line
+        """
+        profiles = self.merged_profiles if use_merged else self.daily_profiles
+        if not profiles:
+            logger.warning("No profiles to generate overlays.")
+            return [], set()
+
+        overlays = []
+        legend_entries = set()
+        full_idx = plot_df.index
+
+        logger.info("Generating overlays for %d profiles (use_merged=%s)", len(profiles), use_merged)
+        for idx, prof in enumerate(profiles):
+            # Robustly get the start timestamp
+            try:
+                start_ts = prof.get("start")
+                if start_ts is None:
+                    start_ts = prof.get("start_date")
+                if start_ts is None:
+                    raise KeyError("Profile missing both 'start' and 'start_date'")
+            except Exception as e:
+                logger.error("Profile %d missing 'start'/'start_date': %s. Skipping overlay.", idx, str(e))
+                continue
+
+            val, vah = prof.get("VAL"), prof.get("VAH")
+            poc = prof.get("POC")
+
+            if val is None or vah is None:
+                logger.warning("Profile %d missing VAL or VAH. Skipping overlay.", idx)
+                continue
+
+            plot_min = plot_df.index.min()
+            if start_ts < plot_min:
+                logger.debug("Adjusting rect start from %s to %s (plot start)", start_ts, plot_min)
+                start_ts = plot_min
+
+            # 1) persistent rectangle for VA zone
+            overlays.append({
+                "kind": "rect",
+                "start": start_ts,
+                "val": val,
+                "vah": vah,
+                "color": "gray",
+                "alpha": 0.2
+            })
+            legend_entries.add(("Value Area", "gray"))
+            logger.debug("Overlay rect: start=%s, VAL=%.2f, VAH=%.2f", start_ts, val, vah)
+
+            # 2) optional POC line as an addplot
+            if poc is not None:
+                poc_series = pd.Series(index=full_idx, dtype=float)
+                poc_series.loc[full_idx >= start_ts] = poc
+
+                ap = make_addplot(poc_series, color="orange", width=1.0, linestyle="--")
+                overlays.append({"kind": "addplot", "plot": ap})
+                legend_entries.add(("POC", "orange"))
+                logger.debug("Overlay POC: start=%s, POC=%.2f", start_ts, poc)
+
+        logger.info("Generated %d overlays", len(overlays))
+        return overlays, legend_entries
+
+    @staticmethod
+    def _calculate_overlap(val1: float, vah1: float, val2: float, vah2: float) -> float:
+        """
+        Compute the overlap ratio between two value areas,
+        normalized by the range of the second area.
+        """
+        low = max(val1, val2)
+        high = min(vah1, vah2)
+        overlap = max(0.0, high - low)
+        range2 = vah2 - val2
+        ratio = overlap / range2 if range2 > 0 else 0.0
+        logger.debug("Calculated overlap: [%.2f, %.2f] vs [%.2f, %.2f] => overlap=%.2f, ratio=%.2f", val1, vah1, val2, vah2, overlap, ratio)
+        return ratio
+
+#Rules below
+
+def breakout_rule(context: Dict, va: Dict) -> List[Dict]:
+    df = context["df"]
+    symbol = context["symbol"]
+    results = []
+
+    logger = logging.getLogger("MarketProfileBreakoutRule")
+    logger.debug("Evaluating breakout_rule for symbol=%s, VA start=%s, VAH=%.2f, VAL=%.2f",
+                 symbol, va.get("start"), va.get("VAH"), va.get("VAL"))
+
+    va_start = va.get("start")
+    curr_time = df.index[-1]
+
+    if va_start is None or (curr_time - va_start) < pd.Timedelta(days=1):
+        logger.debug(
+            "Skipping VA starting at %s: less than 1 day old (age=%s)",
+            va_start, curr_time - va_start
+        )
+        return results
+
+
+    if len(df) < 2:
+        logger.info("Not enough bars in DataFrame (len=%d), skipping breakout evaluation.", len(df))
+        return results
+
+    prev_bar = df.iloc[-2]
+    curr_bar = df.iloc[-1]
+    curr_time = df.index[-1]
+
+    logger.debug(
+        "Prev close=%.2f, Curr close=%.2f, VAH=%.2f, VAL=%.2f",
+        prev_bar["close"], curr_bar["close"], va["VAH"], va["VAL"]
+    )
+
+    if va["VAL"] <= prev_bar["close"] <= va["VAH"]:
+        # Breakout ABOVE
+        if curr_bar["close"] > va["VAH"]:
+            logger.info(
+                "Breakout UP detected: prev_close=%.2f in VA, curr_close=%.2f > VAH=%.2f at %s",
+                prev_bar["close"], curr_bar["close"], va["VAH"], curr_time
+            )
+            results.append({
+                "source": "MarketProfile",
+                "type": "breakout",
+                "symbol": symbol,
+                "time": curr_time,
+                "level_type": "VAH",
+                "distance_pct": round((curr_bar["close"] - va["VAH"]) / va["VAH"], 4),
+                "direction": "up",
+                "trigger_price": curr_bar["close"],
+                "trigger_volume": curr_bar["volume"],
+                "trigger_open": curr_bar["open"],
+                "trigger_high": curr_bar["high"],
+                "trigger_low": curr_bar["low"],
+                "trigger_close": curr_bar["close"],
+                "bar_range": round(curr_bar["high"] - curr_bar["low"], 4),
+                "prev_close": prev_bar["close"],
+                "VAH": va["VAH"],
+                "VAL": va["VAL"],
+                "POC": va.get("POC"),
+                "session_start": va["start"]
+            })
+
+        # Breakout BELOW
+        elif curr_bar["close"] < va["VAL"]:
+            logger.info(
+                "Breakout DOWN detected: prev_close=%.2f in VA, curr_close=%.2f < VAL=%.2f at %s",
+                prev_bar["close"], curr_bar["close"], va["VAL"], curr_time
+            )
+            results.append({
+                "source": "MarketProfile",
+                "type": "breakout",
+                "symbol": symbol,
+                "time": curr_time,
+                "level_type": "VAL",
+                "distance_pct": round((va["VAL"] - curr_bar["close"]) / va["VAL"], 4),
+                "direction": "down",
+                "trigger_price": curr_bar["close"],
+                "trigger_volume": curr_bar["volume"],
+                "trigger_open": curr_bar["open"],
+                "trigger_high": curr_bar["high"],
+                "trigger_low": curr_bar["low"],
+                "trigger_close": curr_bar["close"],
+                "bar_range": round(curr_bar["high"] - curr_bar["low"], 4),
+                "prev_close": prev_bar["close"],
+                "VAH": va["VAH"],
+                "VAL": va["VAL"],
+                "POC": va.get("POC"),
+                "session_start": va["start"]
+            })
+        else:
+            logger.debug(
+                "No breakout: prev_close=%.2f in VA, curr_close=%.2f within VA bounds.",
+                prev_bar["close"], curr_bar["close"]
+            )
+    else:
+        logger.debug(
+            "No breakout: prev_close=%.2f not in VA (VAL=%.2f, VAH=%.2f).",
+            prev_bar["close"], va["VAL"], va["VAH"]
+        )
+
+    logger.debug("Breakout rule produced %d signal(s).", len(results))
+    return results
\ No newline at end of file
diff --git a/classes/indicators/PivotLevelIndicator.py b/src/indicators/pivot_level.py
similarity index 95%
rename from classes/indicators/PivotLevelIndicator.py
rename to src/indicators/pivot_level.py
index 82907c0..8b5ef5e 100644
--- a/classes/indicators/PivotLevelIndicator.py
+++ b/src/indicators/pivot_level.py
@@ -1,11 +1,11 @@
 from dataclasses import dataclass, field
-from classes.indicators.config import DataContext
+from .config import DataContext
 from typing import List, Optional, Tuple, Set
 import pandas as pd
 from mplfinance.plotting import make_addplot
-from classes.Logger import logger
+from core.logger import logger
 from matplotlib import patches
-from classes.indicators.BaseIndicator import BaseIndicator
+from .base import BaseIndicator
 
 @dataclass
 class Level:
diff --git a/classes/indicators/TrendlineIndicator.py b/src/indicators/trendline.py
similarity index 98%
rename from classes/indicators/TrendlineIndicator.py
rename to src/indicators/trendline.py
index ad66b59..5700f8b 100644
--- a/classes/indicators/TrendlineIndicator.py
+++ b/src/indicators/trendline.py
@@ -5,8 +5,8 @@ from typing import List, Tuple, Dict, Set, Literal, Optional
 from scipy.stats import linregress
 from mplfinance.plotting import make_addplot
 from matplotlib import patches
-from classes.indicators.BaseIndicator import BaseIndicator
-from classes.indicators.config import DataContext
+from .base import BaseIndicator
+from .config import DataContext
 
 
 @dataclass
diff --git a/classes/indicators/VWAPIndicator.py b/src/indicators/vwap.py
similarity index 97%
rename from classes/indicators/VWAPIndicator.py
rename to src/indicators/vwap.py
index f568481..d5c0a3f 100644
--- a/classes/indicators/VWAPIndicator.py
+++ b/src/indicators/vwap.py
@@ -1,8 +1,8 @@
 import pandas as pd
 from mplfinance.plotting import make_addplot
 from matplotlib import patches
-from classes.indicators.BaseIndicator import BaseIndicator
-from classes.indicators.config import DataContext
+from indicators.base import BaseIndicator
+from indicators.config import DataContext
 
 
 class VWAPIndicator(BaseIndicator):
diff --git a/src/main.py b/src/main.py
new file mode 100644
index 0000000..206fa17
--- /dev/null
+++ b/src/main.py
@@ -0,0 +1,120 @@
+import pandas as pd
+import time
+from core.logger import logger
+
+from indicators.market_profile import MarketProfileIndicator
+from data_providers.alpaca_provider import AlpacaProvider
+from indicators.config import DataContext
+from signals.engine.market_profile_generator import MarketProfileSignalGenerator
+from dotenv import load_dotenv
+
+# Load environment variables from .env filess
+load_dotenv("secrets.env")
+load_dotenv(".env")
+
+
+symbol = "CL"
+end = "2025-07-15"
+DELAY_SECONDS = 0
+provider = AlpacaProvider()
+
+lower_timespan_ctx = DataContext(
+    symbol=symbol,
+    start="2025-07-01",
+    end=end,
+    interval="15m"
+)
+
+higher_timespan_ctx = DataContext(
+    symbol=symbol,
+    start="2025-05-10",
+    end=end,
+    interval="30m"
+)
+
+def get_value_areas(plot_df: pd.DataFrame):
+    try:
+        logger.info("Creating MarketProfileIndicator with higher timeframe context")
+        mpi = MarketProfileIndicator.from_context(
+            provider=provider,
+            ctx=higher_timespan_ctx,
+            bin_size=0.5,
+            mode="tpo",
+            interval="30m"
+        )
+
+        logger.info("Merging value areas for MarketProfileIndicator")
+        merged = mpi.merge_value_areas()
+        logger.debug("Merged value areas count: %d", len(merged))
+        return merged
+
+    except Exception as e:
+        logger.exception("Error creating value areas: %s", str(e))
+        return []
+
+def simulate_signal_generation(delay_seconds: int = 5):
+    try:
+        logger.info("Fetching OHLCV data for simulation: %s %s-%s (%s)",
+                    lower_timespan_ctx.symbol, lower_timespan_ctx.start, lower_timespan_ctx.end, lower_timespan_ctx.interval)
+        full_df = provider.get_ohlcv(lower_timespan_ctx)
+        if full_df is None or full_df.empty:
+            logger.warning("No data returned for lower timeframe context")
+            return
+
+        logger.info("Fetched data shape: %s", full_df.shape)
+
+        logger.info("Generating value areas and overlays from MarketProfileIndicator.")
+        mpi = MarketProfileIndicator.from_context(
+            provider=provider,
+            ctx=higher_timespan_ctx,
+            bin_size=0.5,
+            mode="tpo",
+            interval="30m"
+        )
+        merged_value_areas = mpi.merge_value_areas()
+        indicator_overlays, legend_entries = mpi.to_overlays(full_df, use_merged=True)
+
+        if not merged_value_areas:
+            logger.warning("No value areas available for signal generation")
+            return
+
+        signal_generator = MarketProfileSignalGenerator(symbol=symbol)
+        all_signals = []
+
+        for i in range(30, len(full_df)):
+            current_df = full_df.iloc[:i+1]
+            signals = signal_generator.generate_signals(current_df, merged_value_areas)
+            all_signals.extend(signals)
+
+            if all_signals:
+                logger.info("[%d] %d new signal(s) at %s", i, len(all_signals), current_df.index[-1])
+                for sig in all_signals:
+                    print(sig.to_dict())
+
+            time.sleep(delay_seconds)
+
+        logger.info("Simulation complete. Total signals generated: %d", len(all_signals))
+
+        signal_overlays = MarketProfileSignalGenerator.to_overlays(plot_df=full_df, signals=all_signals) if all_signals else []
+        combined_overlays = indicator_overlays + signal_overlays
+        legend_entries.add(("MarketProfile breakout", "red")) 
+        chart_title = f"{symbol} – MarketProfileSignal + MarketProfileIndicator – {lower_timespan_ctx.interval}"
+        logger.info("Plotting chart with %d overlays.", len(combined_overlays))
+
+        provider.plot_ohlcv(
+            plot_ctx=lower_timespan_ctx,
+            title=chart_title,
+            overlays=combined_overlays,
+            legend_entries=legend_entries,
+            show_volume=True
+        )
+
+        logger.info("Total signals generated: %d", len(all_signals))
+        logger.info("Simulation completed successfully.")
+
+    except Exception as e:
+        logger.exception("Simulation failed: %s", str(e))
+
+
+if __name__ == "__main__":
+    simulate_signal_generation(DELAY_SECONDS)
diff --git a/src/signals/base.py b/src/signals/base.py
new file mode 100644
index 0000000..0121a9b
--- /dev/null
+++ b/src/signals/base.py
@@ -0,0 +1,22 @@
+# signals/base.py
+
+from dataclasses import dataclass, field
+from typing import Dict, Any
+from datetime import datetime
+
+@dataclass
+class BaseSignal:
+    type: str                 # e.g. "breakout", "retest", etc.
+    symbol: str               # e.g. "CL", "XAUUSD"
+    time: datetime            # timestamp of the signal
+    confidence: float         # 0.0 to 1.0
+    metadata: Dict[str, Any]  # flexible structure for indicator-specific fields
+
+    def to_dict(self):
+        return {
+            "type": self.type,
+            "symbol": self.symbol,
+            "time": self.time.isoformat(),
+            "confidence": self.confidence,
+            "metadata": self.metadata
+        }
diff --git a/src/signals/engine/market_profile_generator.py b/src/signals/engine/market_profile_generator.py
new file mode 100644
index 0000000..6723b0b
--- /dev/null
+++ b/src/signals/engine/market_profile_generator.py
@@ -0,0 +1,120 @@
+from typing import List, Dict
+import pandas as pd
+from indicators.market_profile import breakout_rule
+from mplfinance.plotting import make_addplot
+from signals.base import BaseSignal
+import logging
+
+logger = logging.getLogger("MarketProfileSignalGenerator")
+
+
+class MarketProfileSignalGenerator:
+    def __init__(self, symbol: str):
+        self.symbol = symbol
+
+    def generate_signals(
+        self,
+        df: pd.DataFrame,
+        value_areas: List[Dict]
+    ) -> List[BaseSignal]:
+        """
+        Run all market profile rules against value areas and create BaseSignal objects.
+        """
+        context = {
+            "df": df,
+            "symbol": self.symbol,
+        }
+
+        rules = [
+            breakout_rule,
+            # Add more rule functions here if needed
+        ]
+
+        raw_signals = []
+        for va in value_areas:
+            for rule in rules:
+                new_signals = rule(context, va)
+                raw_signals.extend(new_signals)
+
+                logger.debug(
+                    "INSIGHTS Generated %d signals from rule %s for VA: %s",
+                    len(new_signals), rule.__name__, va
+                )
+
+        logger.info("INSIGHTS Total raw signals generated: %d", len(raw_signals)) 
+
+        signals = [
+            BaseSignal(
+                type=meta["type"],
+                symbol=meta["symbol"],
+                time=meta["time"],
+                confidence=1.0,
+                metadata={k: v for k, v in meta.items() if k not in {"type", "symbol", "time"}}
+            )
+            for meta in raw_signals
+        ]
+
+        logger.info("INSIGHTS Total BaseSignal objects created: %d", len(signals))
+
+        return signals
+
+    @staticmethod
+    def to_overlays(
+        signals: List["BaseSignal"],
+        plot_df: pd.DataFrame,
+        n: int = 3,
+        offset: float = .2,
+    ) -> List[Dict]:
+        overlays = []
+        logger.info("Converting %d signals to line overlays", len(signals))
+
+        for idx, sig in enumerate(signals):
+            if sig.metadata.get("source") != "MarketProfile":
+                logger.debug("Skipping signal %d: not from MarketProfile source", idx)
+                continue
+
+            ts = sig.time
+            if ts not in plot_df.index:
+                nearest_idx = plot_df.index.get_indexer([ts], method="nearest")[0]
+                ts = plot_df.index[nearest_idx]
+
+            direction = sig.metadata.get("direction")
+            base_y = sig.metadata.get("VAH") if sig.metadata.get("level_type") == "VAH" else sig.metadata.get("VAL")
+
+            if base_y is None or direction not in {"up", "down"}:
+                logger.warning("Signal %d missing direction or price level", idx)
+                continue
+
+            # Offset breakout line based on breakout direction
+            y = base_y + offset if direction == "up" else base_y - offset
+
+            center_idx = plot_df.index.get_indexer([ts], method="nearest")[0]
+            start_idx = max(0, center_idx - n)
+            end_idx = min(len(plot_df.index) - 1, center_idx + n)
+
+            short_index = plot_df.index[start_idx:end_idx + 1]
+            line_series = pd.Series(index=plot_df.index, dtype=float)
+            line_series.loc[short_index] = y
+
+            logger.debug(
+                "Signal %d [%s] line from %s to %s at level %.2f",
+                idx, direction, short_index[0], short_index[-1], y
+            )
+
+            ap = make_addplot(
+                line_series,
+                color="green" if direction == "up" else "red",
+                linestyle="-",
+                width=1.0,
+            )
+
+            ap["zorder"] = 6
+
+            overlays.append({
+                "kind": "addplot",
+                "plot": ap,
+                "label": f"Breakout {direction.capitalize()} {idx}"
+            })
+
+        logger.info("Converted %d signals to overlays", len(overlays))
+        return overlays
diff --git a/src/signals/engine/signal_generator.py b/src/signals/engine/signal_generator.py
new file mode 100644
index 0000000..e69de29
diff --git a/classes/logging_utils.py b/src/utils/logging_utils.py
similarity index 100%
rename from classes/logging_utils.py
rename to src/utils/logging_utils.py
diff --git a/tests/conftest.py b/tests/conftest.py
new file mode 100644
index 0000000..e92b7c9
--- /dev/null
+++ b/tests/conftest.py
@@ -0,0 +1,8 @@
+import pytest
+from dotenv import load_dotenv
+
+@pytest.fixture(scope="session", autouse=True)
+def load_env_once():
+    load_dotenv(".env")
+    load_dotenv("secrets.env")
+    print("Environment variables loaded from .env and secrets.env")
diff --git a/tests/test_indicators/test_market_profile_indicator.py b/tests/test_indicators/test_market_profile_indicator.py
index 58cb727..46c651b 100644
--- a/tests/test_indicators/test_market_profile_indicator.py
+++ b/tests/test_indicators/test_market_profile_indicator.py
@@ -2,8 +2,8 @@
 
 import pytest
 import pandas as pd
-from classes.indicators.MarketProfileIndicator import MarketProfileIndicator
-from classes.indicators.config import DataContext
+from indicators.market_profile import MarketProfileIndicator
+from indicators.config import DataContext
 from data_providers.alpaca_provider import AlpacaProvider
 
 
@@ -166,8 +166,8 @@ def test_merge_value_areas_unit(dummy_df):
     # Expect a single merged entry:
     assert len(merged) == 1
     m = merged[0]
-    assert m["start_date"] == day1_start
-    assert m["end_date"]   == day2_end
+    assert m["start"] == day1_start
+    assert m["end"]   == day2_end
     assert m["VAL"] == pytest.approx(99.0)
     assert m["VAH"] == pytest.approx(102.0)
     assert m["POC"] == pytest.approx((100.0 + 101.0) / 2)
diff --git a/tests/test_indicators/test_pivot_level_indicator.py b/tests/test_indicators/test_pivot_level_indicator.py
index 3747a99..d210422 100644
--- a/tests/test_indicators/test_pivot_level_indicator.py
+++ b/tests/test_indicators/test_pivot_level_indicator.py
@@ -1,9 +1,8 @@
 import pytest
 import pandas as pd
-from classes.indicators.PivotLevelIndicator import PivotLevelIndicator
+from indicators.pivot_level import PivotLevelIndicator, Level
 from data_providers.alpaca_provider import AlpacaProvider
-from classes.indicators.PivotLevelIndicator import Level
-from classes.indicators.config import DataContext
+from indicators.config import DataContext
 
 @pytest.mark.integration
 def test_pivot_level_indicator_plot():
diff --git a/tests/test_indicators/test_trendline_indicator.py b/tests/test_indicators/test_trendline_indicator.py
index 22b317c..bd6d830 100644
--- a/tests/test_indicators/test_trendline_indicator.py
+++ b/tests/test_indicators/test_trendline_indicator.py
@@ -1,9 +1,9 @@
 import pytest
 import pandas as pd
 import numpy as np
-from classes.indicators.TrendlineIndicator import TrendlineIndicator, Trendline
+from indicators.trendline import TrendlineIndicator, Trendline
 from data_providers.alpaca_provider import AlpacaProvider
-from classes.indicators.config import DataContext
+from indicators.config import DataContext
 
 @pytest.mark.integration
 def test_trendline_indicator_plot():
diff --git a/tests/test_indicators/test_vwap_indicator.py b/tests/test_indicators/test_vwap_indicator.py
index 363af13..fa87459 100644
--- a/tests/test_indicators/test_vwap_indicator.py
+++ b/tests/test_indicators/test_vwap_indicator.py
@@ -1,8 +1,8 @@
 import pytest
 import pandas as pd
 from data_providers.alpaca_provider import AlpacaProvider
-from classes.indicators.VWAPIndicator import VWAPIndicator
-from classes.indicators.config import DataContext
+from indicators.vwap import VWAPIndicator
+from indicators.config import DataContext
 from matplotlib.patches import Patch
 
 @pytest.mark.integration
